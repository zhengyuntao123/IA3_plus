{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577faf96-3256-48e5-8da4-854831e7006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.0+cu121\n",
      "Torchvision version: 0.19.0+cu121\n",
      "12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:32:02.796687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-29 20:32:02.796718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-29 20:32:02.798270: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 20:32:02.809036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-29 20:32:03.457895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 参考https://medium.com/@edandwe/a-guide-to-craft-your-own-custom-hugging-face-model-ba9cd555a646\n",
    "# 参考https://huggingface.co/docs/transformers/v4.34.0/custom_models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(torch.version.cuda)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TextStreamer, LlamaForCausalLM\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Type, Optional, Tuple\n",
    "from custom_llama.modeling_custom_llama import CustomLlamaModel, CustomLlamaForCausalLM, print_model_layers\n",
    "from custom_llama.configuration_custom_llama import MyLlamaConfig\n",
    "\n",
    "save_directory=\"model/custom_llama\"\n",
    "repo_id = \"yuntaozh/custom_llama\"\n",
    "base_model=\"meta-llama/Llama-3.2-1B\"\n",
    "max_seq_length=5020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d65c3df-a07a-471b-8a34-49287e1efcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册给AutoModel\n",
    "MyLlamaConfig.register_for_auto_class()\n",
    "CustomLlamaModel.register_for_auto_class(\"AutoModel\")\n",
    "CustomLlamaForCausalLM.register_for_auto_class(\"AutoModelForCausalLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf5e725-34d9-476e-ada4-3a7c2b2a8bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_custom_llama.MyLlamaConfig\",\n",
      "    \"AutoModel\": \"modeling_custom_llama.CustomLlamaModel\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_custom_llama.CustomLlamaForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"custom_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llama = AutoModel.from_pretrained(base_model)\n",
    "llama_config = AutoConfig.from_pretrained(base_model)\n",
    "myconfig = MyLlamaConfig(**vars(llama_config))\n",
    "myconfig.auto_map={\n",
    "    \"AutoModel\": \"modeling_custom_llama.CustomLlamaModel\",\n",
    "    \"AutoModelForCausalLM\": \"modeling_custom_llama.CustomLlamaForCausalLM\",\n",
    "    \"AutoConfig\": \"configuration_custom_llama.MyLlamaConfig\"\n",
    "}\n",
    "print(myconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211e3457-7b6e-4345-a3e3-2d25b2019cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'custom_llama.modeling_custom_llama.MyLinear'>\n",
      "embed_tokens: Embedding\n",
      "layers: ModuleList\n",
      "layers.0: LlamaDecoderLayer\n",
      "layers.0.self_attn: LlamaSdpaAttention\n",
      "layers.0.self_attn.q_proj: MyLinear\n",
      "layers.0.self_attn.q_proj.linear: Linear\n",
      "layers.0.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.0.self_attn.k_proj: MyLinear\n",
      "layers.0.self_attn.k_proj.linear: Linear\n",
      "layers.0.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.0.self_attn.v_proj: Linear\n",
      "layers.0.self_attn.o_proj: Linear\n",
      "layers.0.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.0.mlp: LlamaMLP\n",
      "layers.0.mlp.gate_proj: Linear\n",
      "layers.0.mlp.up_proj: Linear\n",
      "layers.0.mlp.down_proj: Linear\n",
      "layers.0.mlp.act_fn: SiLU\n",
      "layers.0.input_layernorm: LlamaRMSNorm\n",
      "layers.0.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.1: LlamaDecoderLayer\n",
      "layers.1.self_attn: LlamaSdpaAttention\n",
      "layers.1.self_attn.q_proj: MyLinear\n",
      "layers.1.self_attn.q_proj.linear: Linear\n",
      "layers.1.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.1.self_attn.k_proj: MyLinear\n",
      "layers.1.self_attn.k_proj.linear: Linear\n",
      "layers.1.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.1.self_attn.v_proj: Linear\n",
      "layers.1.self_attn.o_proj: Linear\n",
      "layers.1.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.1.mlp: LlamaMLP\n",
      "layers.1.mlp.gate_proj: Linear\n",
      "layers.1.mlp.up_proj: Linear\n",
      "layers.1.mlp.down_proj: Linear\n",
      "layers.1.mlp.act_fn: SiLU\n",
      "layers.1.input_layernorm: LlamaRMSNorm\n",
      "layers.1.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.2: LlamaDecoderLayer\n",
      "layers.2.self_attn: LlamaSdpaAttention\n",
      "layers.2.self_attn.q_proj: MyLinear\n",
      "layers.2.self_attn.q_proj.linear: Linear\n",
      "layers.2.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.2.self_attn.k_proj: MyLinear\n",
      "layers.2.self_attn.k_proj.linear: Linear\n",
      "layers.2.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.2.self_attn.v_proj: Linear\n",
      "layers.2.self_attn.o_proj: Linear\n",
      "layers.2.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.2.mlp: LlamaMLP\n",
      "layers.2.mlp.gate_proj: Linear\n",
      "layers.2.mlp.up_proj: Linear\n",
      "layers.2.mlp.down_proj: Linear\n",
      "layers.2.mlp.act_fn: SiLU\n",
      "layers.2.input_layernorm: LlamaRMSNorm\n",
      "layers.2.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.3: LlamaDecoderLayer\n",
      "layers.3.self_attn: LlamaSdpaAttention\n",
      "layers.3.self_attn.q_proj: MyLinear\n",
      "layers.3.self_attn.q_proj.linear: Linear\n",
      "layers.3.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.3.self_attn.k_proj: MyLinear\n",
      "layers.3.self_attn.k_proj.linear: Linear\n",
      "layers.3.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.3.self_attn.v_proj: Linear\n",
      "layers.3.self_attn.o_proj: Linear\n",
      "layers.3.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.3.mlp: LlamaMLP\n",
      "layers.3.mlp.gate_proj: Linear\n",
      "layers.3.mlp.up_proj: Linear\n",
      "layers.3.mlp.down_proj: Linear\n",
      "layers.3.mlp.act_fn: SiLU\n",
      "layers.3.input_layernorm: LlamaRMSNorm\n",
      "layers.3.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.4: LlamaDecoderLayer\n",
      "layers.4.self_attn: LlamaSdpaAttention\n",
      "layers.4.self_attn.q_proj: MyLinear\n",
      "layers.4.self_attn.q_proj.linear: Linear\n",
      "layers.4.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.4.self_attn.k_proj: MyLinear\n",
      "layers.4.self_attn.k_proj.linear: Linear\n",
      "layers.4.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.4.self_attn.v_proj: Linear\n",
      "layers.4.self_attn.o_proj: Linear\n",
      "layers.4.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.4.mlp: LlamaMLP\n",
      "layers.4.mlp.gate_proj: Linear\n",
      "layers.4.mlp.up_proj: Linear\n",
      "layers.4.mlp.down_proj: Linear\n",
      "layers.4.mlp.act_fn: SiLU\n",
      "layers.4.input_layernorm: LlamaRMSNorm\n",
      "layers.4.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.5: LlamaDecoderLayer\n",
      "layers.5.self_attn: LlamaSdpaAttention\n",
      "layers.5.self_attn.q_proj: MyLinear\n",
      "layers.5.self_attn.q_proj.linear: Linear\n",
      "layers.5.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.5.self_attn.k_proj: MyLinear\n",
      "layers.5.self_attn.k_proj.linear: Linear\n",
      "layers.5.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.5.self_attn.v_proj: Linear\n",
      "layers.5.self_attn.o_proj: Linear\n",
      "layers.5.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.5.mlp: LlamaMLP\n",
      "layers.5.mlp.gate_proj: Linear\n",
      "layers.5.mlp.up_proj: Linear\n",
      "layers.5.mlp.down_proj: Linear\n",
      "layers.5.mlp.act_fn: SiLU\n",
      "layers.5.input_layernorm: LlamaRMSNorm\n",
      "layers.5.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.6: LlamaDecoderLayer\n",
      "layers.6.self_attn: LlamaSdpaAttention\n",
      "layers.6.self_attn.q_proj: MyLinear\n",
      "layers.6.self_attn.q_proj.linear: Linear\n",
      "layers.6.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.6.self_attn.k_proj: MyLinear\n",
      "layers.6.self_attn.k_proj.linear: Linear\n",
      "layers.6.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.6.self_attn.v_proj: Linear\n",
      "layers.6.self_attn.o_proj: Linear\n",
      "layers.6.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.6.mlp: LlamaMLP\n",
      "layers.6.mlp.gate_proj: Linear\n",
      "layers.6.mlp.up_proj: Linear\n",
      "layers.6.mlp.down_proj: Linear\n",
      "layers.6.mlp.act_fn: SiLU\n",
      "layers.6.input_layernorm: LlamaRMSNorm\n",
      "layers.6.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.7: LlamaDecoderLayer\n",
      "layers.7.self_attn: LlamaSdpaAttention\n",
      "layers.7.self_attn.q_proj: MyLinear\n",
      "layers.7.self_attn.q_proj.linear: Linear\n",
      "layers.7.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.7.self_attn.k_proj: MyLinear\n",
      "layers.7.self_attn.k_proj.linear: Linear\n",
      "layers.7.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.7.self_attn.v_proj: Linear\n",
      "layers.7.self_attn.o_proj: Linear\n",
      "layers.7.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.7.mlp: LlamaMLP\n",
      "layers.7.mlp.gate_proj: Linear\n",
      "layers.7.mlp.up_proj: Linear\n",
      "layers.7.mlp.down_proj: Linear\n",
      "layers.7.mlp.act_fn: SiLU\n",
      "layers.7.input_layernorm: LlamaRMSNorm\n",
      "layers.7.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.8: LlamaDecoderLayer\n",
      "layers.8.self_attn: LlamaSdpaAttention\n",
      "layers.8.self_attn.q_proj: MyLinear\n",
      "layers.8.self_attn.q_proj.linear: Linear\n",
      "layers.8.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.8.self_attn.k_proj: MyLinear\n",
      "layers.8.self_attn.k_proj.linear: Linear\n",
      "layers.8.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.8.self_attn.v_proj: Linear\n",
      "layers.8.self_attn.o_proj: Linear\n",
      "layers.8.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.8.mlp: LlamaMLP\n",
      "layers.8.mlp.gate_proj: Linear\n",
      "layers.8.mlp.up_proj: Linear\n",
      "layers.8.mlp.down_proj: Linear\n",
      "layers.8.mlp.act_fn: SiLU\n",
      "layers.8.input_layernorm: LlamaRMSNorm\n",
      "layers.8.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.9: LlamaDecoderLayer\n",
      "layers.9.self_attn: LlamaSdpaAttention\n",
      "layers.9.self_attn.q_proj: MyLinear\n",
      "layers.9.self_attn.q_proj.linear: Linear\n",
      "layers.9.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.9.self_attn.k_proj: MyLinear\n",
      "layers.9.self_attn.k_proj.linear: Linear\n",
      "layers.9.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.9.self_attn.v_proj: Linear\n",
      "layers.9.self_attn.o_proj: Linear\n",
      "layers.9.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.9.mlp: LlamaMLP\n",
      "layers.9.mlp.gate_proj: Linear\n",
      "layers.9.mlp.up_proj: Linear\n",
      "layers.9.mlp.down_proj: Linear\n",
      "layers.9.mlp.act_fn: SiLU\n",
      "layers.9.input_layernorm: LlamaRMSNorm\n",
      "layers.9.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.10: LlamaDecoderLayer\n",
      "layers.10.self_attn: LlamaSdpaAttention\n",
      "layers.10.self_attn.q_proj: MyLinear\n",
      "layers.10.self_attn.q_proj.linear: Linear\n",
      "layers.10.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.10.self_attn.k_proj: MyLinear\n",
      "layers.10.self_attn.k_proj.linear: Linear\n",
      "layers.10.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.10.self_attn.v_proj: Linear\n",
      "layers.10.self_attn.o_proj: Linear\n",
      "layers.10.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.10.mlp: LlamaMLP\n",
      "layers.10.mlp.gate_proj: Linear\n",
      "layers.10.mlp.up_proj: Linear\n",
      "layers.10.mlp.down_proj: Linear\n",
      "layers.10.mlp.act_fn: SiLU\n",
      "layers.10.input_layernorm: LlamaRMSNorm\n",
      "layers.10.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.11: LlamaDecoderLayer\n",
      "layers.11.self_attn: LlamaSdpaAttention\n",
      "layers.11.self_attn.q_proj: MyLinear\n",
      "layers.11.self_attn.q_proj.linear: Linear\n",
      "layers.11.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.11.self_attn.k_proj: MyLinear\n",
      "layers.11.self_attn.k_proj.linear: Linear\n",
      "layers.11.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.11.self_attn.v_proj: Linear\n",
      "layers.11.self_attn.o_proj: Linear\n",
      "layers.11.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.11.mlp: LlamaMLP\n",
      "layers.11.mlp.gate_proj: Linear\n",
      "layers.11.mlp.up_proj: Linear\n",
      "layers.11.mlp.down_proj: Linear\n",
      "layers.11.mlp.act_fn: SiLU\n",
      "layers.11.input_layernorm: LlamaRMSNorm\n",
      "layers.11.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.12: LlamaDecoderLayer\n",
      "layers.12.self_attn: LlamaSdpaAttention\n",
      "layers.12.self_attn.q_proj: MyLinear\n",
      "layers.12.self_attn.q_proj.linear: Linear\n",
      "layers.12.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.12.self_attn.k_proj: MyLinear\n",
      "layers.12.self_attn.k_proj.linear: Linear\n",
      "layers.12.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.12.self_attn.v_proj: Linear\n",
      "layers.12.self_attn.o_proj: Linear\n",
      "layers.12.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.12.mlp: LlamaMLP\n",
      "layers.12.mlp.gate_proj: Linear\n",
      "layers.12.mlp.up_proj: Linear\n",
      "layers.12.mlp.down_proj: Linear\n",
      "layers.12.mlp.act_fn: SiLU\n",
      "layers.12.input_layernorm: LlamaRMSNorm\n",
      "layers.12.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.13: LlamaDecoderLayer\n",
      "layers.13.self_attn: LlamaSdpaAttention\n",
      "layers.13.self_attn.q_proj: MyLinear\n",
      "layers.13.self_attn.q_proj.linear: Linear\n",
      "layers.13.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.13.self_attn.k_proj: MyLinear\n",
      "layers.13.self_attn.k_proj.linear: Linear\n",
      "layers.13.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.13.self_attn.v_proj: Linear\n",
      "layers.13.self_attn.o_proj: Linear\n",
      "layers.13.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.13.mlp: LlamaMLP\n",
      "layers.13.mlp.gate_proj: Linear\n",
      "layers.13.mlp.up_proj: Linear\n",
      "layers.13.mlp.down_proj: Linear\n",
      "layers.13.mlp.act_fn: SiLU\n",
      "layers.13.input_layernorm: LlamaRMSNorm\n",
      "layers.13.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.14: LlamaDecoderLayer\n",
      "layers.14.self_attn: LlamaSdpaAttention\n",
      "layers.14.self_attn.q_proj: MyLinear\n",
      "layers.14.self_attn.q_proj.linear: Linear\n",
      "layers.14.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.14.self_attn.k_proj: MyLinear\n",
      "layers.14.self_attn.k_proj.linear: Linear\n",
      "layers.14.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.14.self_attn.v_proj: Linear\n",
      "layers.14.self_attn.o_proj: Linear\n",
      "layers.14.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.14.mlp: LlamaMLP\n",
      "layers.14.mlp.gate_proj: Linear\n",
      "layers.14.mlp.up_proj: Linear\n",
      "layers.14.mlp.down_proj: Linear\n",
      "layers.14.mlp.act_fn: SiLU\n",
      "layers.14.input_layernorm: LlamaRMSNorm\n",
      "layers.14.post_attention_layernorm: LlamaRMSNorm\n",
      "layers.15: LlamaDecoderLayer\n",
      "layers.15.self_attn: LlamaSdpaAttention\n",
      "layers.15.self_attn.q_proj: MyLinear\n",
      "layers.15.self_attn.q_proj.linear: Linear\n",
      "layers.15.self_attn.q_proj.rms_norm: RMSNorm\n",
      "layers.15.self_attn.k_proj: MyLinear\n",
      "layers.15.self_attn.k_proj.linear: Linear\n",
      "layers.15.self_attn.k_proj.rms_norm: RMSNorm\n",
      "layers.15.self_attn.v_proj: Linear\n",
      "layers.15.self_attn.o_proj: Linear\n",
      "layers.15.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "layers.15.mlp: LlamaMLP\n",
      "layers.15.mlp.gate_proj: Linear\n",
      "layers.15.mlp.up_proj: Linear\n",
      "layers.15.mlp.down_proj: Linear\n",
      "layers.15.mlp.act_fn: SiLU\n",
      "layers.15.input_layernorm: LlamaRMSNorm\n",
      "layers.15.post_attention_layernorm: LlamaRMSNorm\n",
      "norm: LlamaRMSNorm\n",
      "rotary_emb: LlamaRotaryEmbedding\n"
     ]
    }
   ],
   "source": [
    "custom_llama = CustomLlamaModel(myconfig)\n",
    "print_model_layers(custom_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b904d9d1-254a-4d2e-b7a0-be444d2bde4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_dict()是一个字典(快照)\n",
    "custom_state_dict = custom_llama.state_dict()\n",
    "\n",
    "for name, param in llama.named_parameters():\n",
    "    source_weight = param.data.clone()\n",
    "    if \"q_proj\" in name or \"k_proj\" in name:\n",
    "        target_name = name.replace(\"weight\", \"linear.weight\")\n",
    "        # print(\"1\",source_weight)\n",
    "        # print(\"2\",custom_llama.state_dict()[target_name])\n",
    "        custom_state_dict[target_name]=source_weight\n",
    "    else:\n",
    "        custom_state_dict[name]=source_weight\n",
    "\n",
    "# 需要重新load才能生效\n",
    "custom_llama.load_state_dict(custom_state_dict)\n",
    "\n",
    "# # 检查是否成功加载参数\n",
    "# for name, param in custom_llama.named_parameters():\n",
    "#     print(name,param)\n",
    "\n",
    "# for name, param in llama.named_parameters():\n",
    "#     print(name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8d3140-a050-4a1f-a7f7-f52fd6638e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/yuntaozh/custom_llama into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "# 先创建huggingface repo，并与本地目录关联\n",
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_id, exist_ok=True)\n",
    "\n",
    "repo = Repository(local_dir=save_directory, clone_from=repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9fdec2-da85-41e5-8b29-bf940f4e8636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/yuntaozh/custom_llama\n",
      "   3453744..11b63b5  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/yuntaozh/custom_llama/commit/11b63b5d3e3277ecbc57f014efc2fd398e239920'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# git lfs追踪大文件\n",
    "repo.lfs_track([\"*.json\", \"*.safetensors\"])\n",
    "\n",
    "repo.git_add(\".gitattributes\")\n",
    "repo.git_commit(\"Modify git lfs\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2656fded-bd5b-401b-8b6b-486058415193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "# # 不加这两句话 trainer.train()可能报错\n",
    "# if tokenizer.pad_token_id is None:\n",
    "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# if model.config.pad_token_id is None:\n",
    "#     model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644cb4cf-9a23-4270-8c75-fa56b78c8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_llama.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "myconfig.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a11e6429-4db2-47b5-ab2a-112029fcc9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/custom_llama/__init__.py'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copy(\"custom_llama/modeling_custom_llama.py\", save_directory)\n",
    "shutil.copy(\"custom_llama/configuration_custom_llama.py\", save_directory)\n",
    "shutil.copy(\"custom_llama/__init__.py\", save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b661c132-4eb6-43f3-bee2-cdb47caa4d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1137c2e26a45928d967cf5ea4081a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/4.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f6c6acda31437285de8e506df30e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer.json:   0%|          | 1.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80bdf4d964547cbb27d4506c14dc9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file special_tokens_map.json:   0%|          | 1.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f96715a94b9400dbb3fb5340800c1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file tokenizer_config.json:   0%|          | 1.00/49.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdba3aaccfe4afcb5cd5f63436d21df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file config.json:   0%|          | 1.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/yuntaozh/custom_llama\n",
      "   11b63b5..ab192f6  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/yuntaozh/custom_llama/commit/ab192f6037e0cb576942f0e6ff9ed9071dddb3ef'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Add all files and push\n",
    "repo.git_add()\n",
    "repo.git_commit(\"Upload custom_llama\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b76aa41-2051-40eb-97eb-c4d1d9e49daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import HfApi, HfFolder, Repository\n",
    "\n",
    "# save_directory=\"model/ours_pretrained\"\n",
    "# api = HfApi()\n",
    "\n",
    "# repo = Repository(local_dir=save_directory, clone_from=repo_id)\n",
    "\n",
    "# # # Add all files and push\n",
    "# repo.git_add()\n",
    "# repo.git_commit(\"Update config\")\n",
    "# repo.git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8474ee67-aad3-40a2-819f-1e3098dfe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
    "# config = AutoConfig.from_pretrained(\n",
    "#     \"model/ours_pretrained\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# # model = AutoModel.from_config(config, trust_remote_code=True)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"model/ours_pretrained\",\n",
    "#     trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a274a18-903f-4ea2-bcf4-c5235ee54653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44859c60ef6148c8b2bd7068549a00f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/yuntaozh/custom_llama:\n",
      "- configuration_custom_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyLlamaConfig {\n",
      "  \"_name_or_path\": \"yuntaozh/custom_llama\",\n",
      "  \"architectures\": [\n",
      "    \"CustomLlamaModel\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"yuntaozh/custom_llama--configuration_custom_llama.MyLlamaConfig\",\n",
      "    \"AutoModel\": \"yuntaozh/custom_llama--modeling_custom_llama.CustomLlamaModel\",\n",
      "    \"AutoModelForCausalLM\": \"yuntaozh/custom_llama--modeling_custom_llama.CustomLlamaForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"custom_llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"yuntaozh/custom_llama\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12eeeae9-c966-4c58-a590-59ab5df983ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n",
      "Replacing:  Linear <class 'transformers_modules.yuntaozh.custom_llama.ab192f6037e0cb576942f0e6ff9ed9071dddb3ef.modeling_custom_llama.MyLinear'>\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yuntaozh/custom_llama\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "512ac4e0-46c5-40ef-b0f2-8ec05b3a6889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: CustomLlamaModel\n",
      "model.embed_tokens: Embedding\n",
      "model.layers: ModuleList\n",
      "model.layers.0: LlamaDecoderLayer\n",
      "model.layers.0.self_attn: LlamaSdpaAttention\n",
      "model.layers.0.self_attn.q_proj: MyLinear\n",
      "model.layers.0.self_attn.q_proj.linear: Linear\n",
      "model.layers.0.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.0.self_attn.k_proj: MyLinear\n",
      "model.layers.0.self_attn.k_proj.linear: Linear\n",
      "model.layers.0.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.0.self_attn.v_proj: Linear\n",
      "model.layers.0.self_attn.o_proj: Linear\n",
      "model.layers.0.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.0.mlp: LlamaMLP\n",
      "model.layers.0.mlp.gate_proj: Linear\n",
      "model.layers.0.mlp.up_proj: Linear\n",
      "model.layers.0.mlp.down_proj: Linear\n",
      "model.layers.0.mlp.act_fn: SiLU\n",
      "model.layers.0.input_layernorm: LlamaRMSNorm\n",
      "model.layers.0.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.1: LlamaDecoderLayer\n",
      "model.layers.1.self_attn: LlamaSdpaAttention\n",
      "model.layers.1.self_attn.q_proj: MyLinear\n",
      "model.layers.1.self_attn.q_proj.linear: Linear\n",
      "model.layers.1.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.1.self_attn.k_proj: MyLinear\n",
      "model.layers.1.self_attn.k_proj.linear: Linear\n",
      "model.layers.1.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.1.self_attn.v_proj: Linear\n",
      "model.layers.1.self_attn.o_proj: Linear\n",
      "model.layers.1.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.1.mlp: LlamaMLP\n",
      "model.layers.1.mlp.gate_proj: Linear\n",
      "model.layers.1.mlp.up_proj: Linear\n",
      "model.layers.1.mlp.down_proj: Linear\n",
      "model.layers.1.mlp.act_fn: SiLU\n",
      "model.layers.1.input_layernorm: LlamaRMSNorm\n",
      "model.layers.1.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.2: LlamaDecoderLayer\n",
      "model.layers.2.self_attn: LlamaSdpaAttention\n",
      "model.layers.2.self_attn.q_proj: MyLinear\n",
      "model.layers.2.self_attn.q_proj.linear: Linear\n",
      "model.layers.2.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.2.self_attn.k_proj: MyLinear\n",
      "model.layers.2.self_attn.k_proj.linear: Linear\n",
      "model.layers.2.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.2.self_attn.v_proj: Linear\n",
      "model.layers.2.self_attn.o_proj: Linear\n",
      "model.layers.2.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.2.mlp: LlamaMLP\n",
      "model.layers.2.mlp.gate_proj: Linear\n",
      "model.layers.2.mlp.up_proj: Linear\n",
      "model.layers.2.mlp.down_proj: Linear\n",
      "model.layers.2.mlp.act_fn: SiLU\n",
      "model.layers.2.input_layernorm: LlamaRMSNorm\n",
      "model.layers.2.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.3: LlamaDecoderLayer\n",
      "model.layers.3.self_attn: LlamaSdpaAttention\n",
      "model.layers.3.self_attn.q_proj: MyLinear\n",
      "model.layers.3.self_attn.q_proj.linear: Linear\n",
      "model.layers.3.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.3.self_attn.k_proj: MyLinear\n",
      "model.layers.3.self_attn.k_proj.linear: Linear\n",
      "model.layers.3.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.3.self_attn.v_proj: Linear\n",
      "model.layers.3.self_attn.o_proj: Linear\n",
      "model.layers.3.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.3.mlp: LlamaMLP\n",
      "model.layers.3.mlp.gate_proj: Linear\n",
      "model.layers.3.mlp.up_proj: Linear\n",
      "model.layers.3.mlp.down_proj: Linear\n",
      "model.layers.3.mlp.act_fn: SiLU\n",
      "model.layers.3.input_layernorm: LlamaRMSNorm\n",
      "model.layers.3.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.4: LlamaDecoderLayer\n",
      "model.layers.4.self_attn: LlamaSdpaAttention\n",
      "model.layers.4.self_attn.q_proj: MyLinear\n",
      "model.layers.4.self_attn.q_proj.linear: Linear\n",
      "model.layers.4.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.4.self_attn.k_proj: MyLinear\n",
      "model.layers.4.self_attn.k_proj.linear: Linear\n",
      "model.layers.4.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.4.self_attn.v_proj: Linear\n",
      "model.layers.4.self_attn.o_proj: Linear\n",
      "model.layers.4.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.4.mlp: LlamaMLP\n",
      "model.layers.4.mlp.gate_proj: Linear\n",
      "model.layers.4.mlp.up_proj: Linear\n",
      "model.layers.4.mlp.down_proj: Linear\n",
      "model.layers.4.mlp.act_fn: SiLU\n",
      "model.layers.4.input_layernorm: LlamaRMSNorm\n",
      "model.layers.4.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.5: LlamaDecoderLayer\n",
      "model.layers.5.self_attn: LlamaSdpaAttention\n",
      "model.layers.5.self_attn.q_proj: MyLinear\n",
      "model.layers.5.self_attn.q_proj.linear: Linear\n",
      "model.layers.5.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.5.self_attn.k_proj: MyLinear\n",
      "model.layers.5.self_attn.k_proj.linear: Linear\n",
      "model.layers.5.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.5.self_attn.v_proj: Linear\n",
      "model.layers.5.self_attn.o_proj: Linear\n",
      "model.layers.5.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.5.mlp: LlamaMLP\n",
      "model.layers.5.mlp.gate_proj: Linear\n",
      "model.layers.5.mlp.up_proj: Linear\n",
      "model.layers.5.mlp.down_proj: Linear\n",
      "model.layers.5.mlp.act_fn: SiLU\n",
      "model.layers.5.input_layernorm: LlamaRMSNorm\n",
      "model.layers.5.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.6: LlamaDecoderLayer\n",
      "model.layers.6.self_attn: LlamaSdpaAttention\n",
      "model.layers.6.self_attn.q_proj: MyLinear\n",
      "model.layers.6.self_attn.q_proj.linear: Linear\n",
      "model.layers.6.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.6.self_attn.k_proj: MyLinear\n",
      "model.layers.6.self_attn.k_proj.linear: Linear\n",
      "model.layers.6.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.6.self_attn.v_proj: Linear\n",
      "model.layers.6.self_attn.o_proj: Linear\n",
      "model.layers.6.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.6.mlp: LlamaMLP\n",
      "model.layers.6.mlp.gate_proj: Linear\n",
      "model.layers.6.mlp.up_proj: Linear\n",
      "model.layers.6.mlp.down_proj: Linear\n",
      "model.layers.6.mlp.act_fn: SiLU\n",
      "model.layers.6.input_layernorm: LlamaRMSNorm\n",
      "model.layers.6.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.7: LlamaDecoderLayer\n",
      "model.layers.7.self_attn: LlamaSdpaAttention\n",
      "model.layers.7.self_attn.q_proj: MyLinear\n",
      "model.layers.7.self_attn.q_proj.linear: Linear\n",
      "model.layers.7.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.7.self_attn.k_proj: MyLinear\n",
      "model.layers.7.self_attn.k_proj.linear: Linear\n",
      "model.layers.7.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.7.self_attn.v_proj: Linear\n",
      "model.layers.7.self_attn.o_proj: Linear\n",
      "model.layers.7.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.7.mlp: LlamaMLP\n",
      "model.layers.7.mlp.gate_proj: Linear\n",
      "model.layers.7.mlp.up_proj: Linear\n",
      "model.layers.7.mlp.down_proj: Linear\n",
      "model.layers.7.mlp.act_fn: SiLU\n",
      "model.layers.7.input_layernorm: LlamaRMSNorm\n",
      "model.layers.7.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.8: LlamaDecoderLayer\n",
      "model.layers.8.self_attn: LlamaSdpaAttention\n",
      "model.layers.8.self_attn.q_proj: MyLinear\n",
      "model.layers.8.self_attn.q_proj.linear: Linear\n",
      "model.layers.8.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.8.self_attn.k_proj: MyLinear\n",
      "model.layers.8.self_attn.k_proj.linear: Linear\n",
      "model.layers.8.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.8.self_attn.v_proj: Linear\n",
      "model.layers.8.self_attn.o_proj: Linear\n",
      "model.layers.8.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.8.mlp: LlamaMLP\n",
      "model.layers.8.mlp.gate_proj: Linear\n",
      "model.layers.8.mlp.up_proj: Linear\n",
      "model.layers.8.mlp.down_proj: Linear\n",
      "model.layers.8.mlp.act_fn: SiLU\n",
      "model.layers.8.input_layernorm: LlamaRMSNorm\n",
      "model.layers.8.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.9: LlamaDecoderLayer\n",
      "model.layers.9.self_attn: LlamaSdpaAttention\n",
      "model.layers.9.self_attn.q_proj: MyLinear\n",
      "model.layers.9.self_attn.q_proj.linear: Linear\n",
      "model.layers.9.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.9.self_attn.k_proj: MyLinear\n",
      "model.layers.9.self_attn.k_proj.linear: Linear\n",
      "model.layers.9.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.9.self_attn.v_proj: Linear\n",
      "model.layers.9.self_attn.o_proj: Linear\n",
      "model.layers.9.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.9.mlp: LlamaMLP\n",
      "model.layers.9.mlp.gate_proj: Linear\n",
      "model.layers.9.mlp.up_proj: Linear\n",
      "model.layers.9.mlp.down_proj: Linear\n",
      "model.layers.9.mlp.act_fn: SiLU\n",
      "model.layers.9.input_layernorm: LlamaRMSNorm\n",
      "model.layers.9.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.10: LlamaDecoderLayer\n",
      "model.layers.10.self_attn: LlamaSdpaAttention\n",
      "model.layers.10.self_attn.q_proj: MyLinear\n",
      "model.layers.10.self_attn.q_proj.linear: Linear\n",
      "model.layers.10.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.10.self_attn.k_proj: MyLinear\n",
      "model.layers.10.self_attn.k_proj.linear: Linear\n",
      "model.layers.10.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.10.self_attn.v_proj: Linear\n",
      "model.layers.10.self_attn.o_proj: Linear\n",
      "model.layers.10.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.10.mlp: LlamaMLP\n",
      "model.layers.10.mlp.gate_proj: Linear\n",
      "model.layers.10.mlp.up_proj: Linear\n",
      "model.layers.10.mlp.down_proj: Linear\n",
      "model.layers.10.mlp.act_fn: SiLU\n",
      "model.layers.10.input_layernorm: LlamaRMSNorm\n",
      "model.layers.10.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.11: LlamaDecoderLayer\n",
      "model.layers.11.self_attn: LlamaSdpaAttention\n",
      "model.layers.11.self_attn.q_proj: MyLinear\n",
      "model.layers.11.self_attn.q_proj.linear: Linear\n",
      "model.layers.11.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.11.self_attn.k_proj: MyLinear\n",
      "model.layers.11.self_attn.k_proj.linear: Linear\n",
      "model.layers.11.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.11.self_attn.v_proj: Linear\n",
      "model.layers.11.self_attn.o_proj: Linear\n",
      "model.layers.11.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.11.mlp: LlamaMLP\n",
      "model.layers.11.mlp.gate_proj: Linear\n",
      "model.layers.11.mlp.up_proj: Linear\n",
      "model.layers.11.mlp.down_proj: Linear\n",
      "model.layers.11.mlp.act_fn: SiLU\n",
      "model.layers.11.input_layernorm: LlamaRMSNorm\n",
      "model.layers.11.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.12: LlamaDecoderLayer\n",
      "model.layers.12.self_attn: LlamaSdpaAttention\n",
      "model.layers.12.self_attn.q_proj: MyLinear\n",
      "model.layers.12.self_attn.q_proj.linear: Linear\n",
      "model.layers.12.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.12.self_attn.k_proj: MyLinear\n",
      "model.layers.12.self_attn.k_proj.linear: Linear\n",
      "model.layers.12.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.12.self_attn.v_proj: Linear\n",
      "model.layers.12.self_attn.o_proj: Linear\n",
      "model.layers.12.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.12.mlp: LlamaMLP\n",
      "model.layers.12.mlp.gate_proj: Linear\n",
      "model.layers.12.mlp.up_proj: Linear\n",
      "model.layers.12.mlp.down_proj: Linear\n",
      "model.layers.12.mlp.act_fn: SiLU\n",
      "model.layers.12.input_layernorm: LlamaRMSNorm\n",
      "model.layers.12.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.13: LlamaDecoderLayer\n",
      "model.layers.13.self_attn: LlamaSdpaAttention\n",
      "model.layers.13.self_attn.q_proj: MyLinear\n",
      "model.layers.13.self_attn.q_proj.linear: Linear\n",
      "model.layers.13.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.13.self_attn.k_proj: MyLinear\n",
      "model.layers.13.self_attn.k_proj.linear: Linear\n",
      "model.layers.13.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.13.self_attn.v_proj: Linear\n",
      "model.layers.13.self_attn.o_proj: Linear\n",
      "model.layers.13.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.13.mlp: LlamaMLP\n",
      "model.layers.13.mlp.gate_proj: Linear\n",
      "model.layers.13.mlp.up_proj: Linear\n",
      "model.layers.13.mlp.down_proj: Linear\n",
      "model.layers.13.mlp.act_fn: SiLU\n",
      "model.layers.13.input_layernorm: LlamaRMSNorm\n",
      "model.layers.13.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.14: LlamaDecoderLayer\n",
      "model.layers.14.self_attn: LlamaSdpaAttention\n",
      "model.layers.14.self_attn.q_proj: MyLinear\n",
      "model.layers.14.self_attn.q_proj.linear: Linear\n",
      "model.layers.14.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.14.self_attn.k_proj: MyLinear\n",
      "model.layers.14.self_attn.k_proj.linear: Linear\n",
      "model.layers.14.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.14.self_attn.v_proj: Linear\n",
      "model.layers.14.self_attn.o_proj: Linear\n",
      "model.layers.14.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.14.mlp: LlamaMLP\n",
      "model.layers.14.mlp.gate_proj: Linear\n",
      "model.layers.14.mlp.up_proj: Linear\n",
      "model.layers.14.mlp.down_proj: Linear\n",
      "model.layers.14.mlp.act_fn: SiLU\n",
      "model.layers.14.input_layernorm: LlamaRMSNorm\n",
      "model.layers.14.post_attention_layernorm: LlamaRMSNorm\n",
      "model.layers.15: LlamaDecoderLayer\n",
      "model.layers.15.self_attn: LlamaSdpaAttention\n",
      "model.layers.15.self_attn.q_proj: MyLinear\n",
      "model.layers.15.self_attn.q_proj.linear: Linear\n",
      "model.layers.15.self_attn.q_proj.rms_norm: RMSNorm\n",
      "model.layers.15.self_attn.k_proj: MyLinear\n",
      "model.layers.15.self_attn.k_proj.linear: Linear\n",
      "model.layers.15.self_attn.k_proj.rms_norm: RMSNorm\n",
      "model.layers.15.self_attn.v_proj: Linear\n",
      "model.layers.15.self_attn.o_proj: Linear\n",
      "model.layers.15.self_attn.rotary_emb: LlamaRotaryEmbedding\n",
      "model.layers.15.mlp: LlamaMLP\n",
      "model.layers.15.mlp.gate_proj: Linear\n",
      "model.layers.15.mlp.up_proj: Linear\n",
      "model.layers.15.mlp.down_proj: Linear\n",
      "model.layers.15.mlp.act_fn: SiLU\n",
      "model.layers.15.input_layernorm: LlamaRMSNorm\n",
      "model.layers.15.post_attention_layernorm: LlamaRMSNorm\n",
      "model.norm: LlamaRMSNorm\n",
      "model.rotary_emb: LlamaRotaryEmbedding\n",
      "lm_head: Linear\n"
     ]
    }
   ],
   "source": [
    "print_model_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2815b27a-34c9-4b90-8652-a2896bbdccdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomLlamaForCausalLM\n"
     ]
    }
   ],
   "source": [
    "print(model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070c826-1c1d-450c-9e36-9ece86240080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
